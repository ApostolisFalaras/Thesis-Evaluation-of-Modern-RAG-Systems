{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e545129-5827-4a28-aed3-807d7545623e",
   "metadata": {},
   "source": [
    "# Evaluation of a RAG Pipeline using the LegalBench-RAG dataset\n",
    "\n",
    "This notebook presents the implementation of a simple Retrieval-Augmented Generation (RAG) pipeline, and its evaluation using advanced frameworks that provide a collection of metrics that assess the performance of the retrieval and generation components of the pipeline, such as **RAGChecker**, **ARES**, **RAGAs**, and **AutoRAG**.\n",
    "\n",
    "The pipeline will be developed using the **LangChain** framework, employing the **SQLiteVec** vector database, that will store the corpus embeddings and act both as the vector store and the retrieval component, and OpenAI's `GPT-4o mini` model as the generator component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c894e-a073-4ece-a8fe-0d963113eafb",
   "metadata": {},
   "source": [
    "## 1. Inserting API Keys\n",
    "\n",
    "* LangChain API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a856b8-2ea7-4de0-8265-8d99accc5ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert LangChain API Key: ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Insert LangChain API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca998c-443d-4d6c-8872-37931230fadb",
   "metadata": {},
   "source": [
    "* OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e53f91c0-ad5c-4ff9-b759-c94c2acea8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert OpenAI API key: ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Insert OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac6037-0c13-4106-9b77-097f38cd3e72",
   "metadata": {},
   "source": [
    "## 2 Dataset Preprocessing\n",
    "\n",
    "LegalBench-RAG is the legal benchmark that assesses RAG systems on their capacity to understand legal jargon and retrieve pertinent (to a question) legal documents, and compiles legal documents from 4 datasets:\n",
    "\n",
    "**a)** Privacy Question-Answering (**PrivacyQA**): PrivacyQA consists of 1,750 questions about the contents of privacy policies of mobile applications, with over 3,500 annotations from experts.\n",
    "\n",
    "**b)** Contract Understanding Atticus Dataset (**CUAD**): CUAD consists of more than 500 legal documents and more than 13,000 annotations, made by legal experts that are members of the Atticus Project (a non-profit organization of legal experts). The documents extend up to 41 label categories. \n",
    "\n",
    "**c)** Merger Agreement Understanding Dataset (**MAUD**): MAUD is a reading comprehension dataset including over 39,000 examples and over 47,000 annotations, originating from the American Bar Association's 2021 Public Target Deal Points Study.\n",
    "\n",
    "**d)** Contract Natural Language Inference (**ContractNLI**): ContractNLI contains 607 legal contracts and addresses the contract review automation task. The system is assigned to figure out whether a set of hypotheses can be entailed in a specific document.\n",
    "\n",
    "The LegalBench-RAG dataset contains 4 folders that each consists of a set of .txt legal documents that I preprocess into `Document` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c1c1514-8b26-468b-a096-3c6b2716611a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Documents created: 698\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# List of input corpus directories\n",
    "directories = [\"./Datasets/contractnli/\", \"./Datasets/cuad/\", \"./Datasets/maud/\", \"./Datasets/privacy_qa/\"]\n",
    "\n",
    "legalbench_rag_retrieval_docs = []\n",
    "\n",
    "# For every folder in the input corpus\n",
    "for folder in directories:\n",
    "\n",
    "    # For every .txt file in each folder\n",
    "    for file in os.listdir(folder):\n",
    "\n",
    "        # Making sure that we only account for text files\n",
    "        if file.endswith(\".txt\"):\n",
    "\n",
    "            filepath = folder + file\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                doc = Document(\n",
    "                    page_content=f.read(),\n",
    "                    metadata={\"source_dataset\": folder, \"filename\": file}\n",
    "                )\n",
    "\n",
    "                legalbench_rag_retrieval_docs.append(doc)\n",
    "\n",
    "print(f\"Total number of Documents created: {len(legalbench_rag_retrieval_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bd8b3f-0332-4e4a-b12b-9e908a2ee002",
   "metadata": {},
   "source": [
    "Also, the dataset includes 4 .json files for each folder, that contain test cases of questions and answer spans from a document. I extract the QA pairs and store them in a list, as dictionaries. Additionally, I assign an arbitrary `query_id` field to each QA pair, based on the order they're accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c141b27-8dbf-45fa-9265-66729e02fe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of question-answer pairs created: 6889\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "legalbench_rag_qa = []\n",
    "\n",
    "benchmarks = [\"./Datasets/benchmarks/contractnli.json\", \"./Datasets/benchmarks/cuad.json\", \n",
    "              \"./Datasets/benchmarks/maud.json\", \"./Datasets/benchmarks/privacy_qa.json\"]\n",
    "\n",
    "query_id = 1\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "\n",
    "    # I load each .json file\n",
    "    with open(benchmark, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "        # Each test instance contains a query,\n",
    "        for example in data[\"tests\"]:\n",
    "            query = example[\"query\"]\n",
    "\n",
    "            answers = []\n",
    "            # and a set of answer snippets with corresponding source file\n",
    "            for snippet in example[\"snippets\"]:\n",
    "                answers.append(snippet[\"answer\"] + \"\\nSource: \" + snippet[\"file_path\"])\n",
    "                \n",
    "            legalbench_rag_qa.append({\"query_id\": query_id, \"query\": query, \"answers\": answers})\n",
    "            query_id += 1\n",
    "\n",
    "print(f\"Total number of question-answer pairs created: {len(legalbench_rag_qa)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519cb09-4f19-41f1-93c8-5191894abdb5",
   "metadata": {},
   "source": [
    "### 2.1 Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93bc9804-7611-463e-80ef-ea3cbafd940a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average size (in terms of characters) of the legal documents is: 105009.388252149\n"
     ]
    }
   ],
   "source": [
    "total_length = 0\n",
    "\n",
    "for doc in legalbench_rag_retrieval_docs:\n",
    "    total_length += len(doc.page_content)\n",
    "\n",
    "print(f\"The average size (in terms of characters) of the legal documents is: {total_length/len(legalbench_rag_retrieval_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3aa465-f041-41c2-8f75-8c5eb3be9ac7",
   "metadata": {},
   "source": [
    "The average size of the legal documents is approximately 105,000 characters, while GPT-4o mini has a context window of 128,000 tokens. Retrieving the top-10 most similar context chunks for a query could easily exceed this context window limit, making it impractical for effective processing. That's why I decided to split the retrieval corpus into chunks of 2,000 characters with a 500-character overlap, ensuring that the retrieved text remains within the model's context window while preserving contextual coherence. \n",
    "\n",
    "For the chunking process, I use LangChain's `RecursiveCharacterTextSplitter` that splits the input `Document` objects based on the number of specified tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d54d8a6b-df4d-49d9-860b-a0e51a7b1ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 58387\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=500, add_start_index=True)\n",
    "\n",
    "legalbench_rag_chunks = text_splitter.split_documents(legalbench_rag_retrieval_docs)\n",
    "\n",
    "print(f\"Total number of chunks: {len(legalbench_rag_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea244cc-4d3d-40c6-bfa7-aee851c71887",
   "metadata": {},
   "source": [
    "## 3. SQLiteVec Vector Store\n",
    "\n",
    "My RAG pipeline employs a SQLiteVec vector store that uses OpenAI's `text-embedding-3-large` to embed and store the chunk embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04df20e2-ed43-4ec7-8a5f-6153d639acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e73598-6e71-4834-a71b-028a563e8458",
   "metadata": {},
   "source": [
    "**NOTE:** Unlike the Chroma vector store that will check if the vector store is existent in the specified directory, and if it's there, the vector store will be instantiated, SQLiteVec will try to recreate the database from the beginning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c80ee-a8b7-4c2d-af26-94c63ac90131",
   "metadata": {},
   "source": [
    "**Small Typo Error I noticed after testing - The print() functions, in the cell below, should mention the SQLiteVec instead of the Chroma vector store.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de995734-d724-4987-af92-7c8070de56bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma vector store is empty. Inserting chunk embeddings....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 59/59 [31:00<00:00, 31.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk embeddings added to the vector store.\n",
      "Chroma vector store initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import SQLiteVec\n",
    "from tqdm import tqdm\n",
    "from uuid import uuid4\n",
    "import os\n",
    "\n",
    "# Initialization of the SQLiteVec vector store\n",
    "legalbench_rag_vector_store = SQLiteVec(\n",
    "    table=\"LegalBench_RAG\",\n",
    "    db_file=\"./Vector_Stores/LegalBench_RAG/legalbench_rag.db\",\n",
    "    embedding=embeddings,\n",
    "    connection=None\n",
    ")\n",
    "\n",
    "print(\"Chroma vector store is empty. Inserting chunk embeddings....\")\n",
    "    \n",
    "# creating a unique identifier for each dataset chunk to be stored\n",
    "uuids = [str(uuid4()) for _ in range(len(legalbench_rag_chunks))]\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "# Lists storing the text passages and their corresponding metadata\n",
    "batch_texts = []\n",
    "batch_metadata = []\n",
    "\n",
    "for i in tqdm(range(0, len(legalbench_rag_chunks), batch_size)):\n",
    "    batch_texts = [chunk.page_content for chunk in legalbench_rag_chunks[i:i+batch_size]]\n",
    "    batch_metadata = [chunk.metadata for chunk in legalbench_rag_chunks[i:i+batch_size]]\n",
    "    \n",
    "    legalbench_rag_vector_store.add_texts(\n",
    "        texts=batch_texts,\n",
    "        metdata=batch_metadata\n",
    "    )\n",
    "print(\"Chunk embeddings added to the vector store.\")\n",
    "\n",
    "print(\"Chroma vector store initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d190c1c-6d08-412f-9342-451e27e57293",
   "metadata": {},
   "source": [
    "## 4. Testing the RAG pipeline\n",
    "\n",
    "In this section, I define the retrieval and generation functionalities. Then, I query the SQLiteVec vector stores that also acts as the retriever.\n",
    "\n",
    "Given that the LegalBench-RAG training set contains 6889 instances and that SQLiteVec doesn't support parallel execution of queries, I randomly sampled 100 QA instances dataset, by selecting their corresponding indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "571939b6-3042-4a9f-99f3-7e93d11b9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Setting the seed for reproducibility of the experiments\n",
    "random.seed(100)\n",
    "\n",
    "sampled_indices = random.sample(range(6889), 100)\n",
    "\n",
    "# Defining the list of remaining available indices that will later be used in the ARES evaluation process \n",
    "available_indices = set(range(6889)) - set(sampled_indices)\n",
    "\n",
    "legalbench_rag_subset = []\n",
    "\n",
    "for index in sampled_indices:\n",
    "    legalbench_rag_subset.append(legalbench_rag_qa[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a7039-8bec-4edf-b833-89d9c88f76b3",
   "metadata": {},
   "source": [
    "* At this point, I define a `results` list that stores dictionaries of (query, query_id, ground-truth answer, generated answer, retrieved context passages) instances of the sampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63f6423-ccab-4fea-8974-c0842f0fa5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the RAG pipeline and collecting the retrieved passages and generated answers....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [10:50<00:00,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing process is completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import SQLiteVec\n",
    "from langchain import hub\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Instantiating the LegalBench-RAG's SQLiteVec vector store\n",
    "connection = SQLiteVec.create_connection(db_file=\"./Vector_Stores/LegalBench_RAG/legalbench_rag.db\")\n",
    "\n",
    "legalbench_rag_vector_store = SQLiteVec(\n",
    "    table=\"LegalBench_RAG\",\n",
    "    embedding=embeddings,\n",
    "    connection=connection\n",
    ")\n",
    "\n",
    "# Defining the prompt template of the RAG pipeline\n",
    "prompt_template = \"\"\"Answer the following question:\n",
    "\\n\\n\n",
    "{question}\n",
    "\\n\\n\n",
    "\n",
    "Using the following list of context passages:\n",
    "\\n\\n\n",
    "{context}\n",
    "\\n\\n\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "\n",
    "# Instantiating the generator model GPT-4o mini\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "# RAG PIPELINE\n",
    "print(\"Testing the RAG pipeline and collecting the retrieved passages and generated answers....\")\n",
    "results = []\n",
    "\n",
    "for qa in tqdm(legalbench_rag_subset):\n",
    "    # RETRIEVAL\n",
    "    retrieved_chunks = legalbench_rag_vector_store.similarity_search(qa[\"query\"], k=10)\n",
    "    \n",
    "    # Processing the retrieved chunks, by adding their url source in the end of the text segment,\n",
    "    # so that i only add the important text of each chunk, and not the metadata.\n",
    "    retrieved_context = \"\"\n",
    "    for context in retrieved_chunks:\n",
    "        retrieved_context += context.page_content + \"\\n\\n\"\n",
    "\n",
    "    # GENERATION\n",
    "    # Configuring the input prompt and calling GPT-4o mini, by promting it.\n",
    "    prompt_message = prompt.format_prompt(question=qa[\"query\"], context=retrieved_context)\n",
    "    response = generator_llm.invoke(prompt_message)\n",
    "\n",
    "    result = {\n",
    "        \"query_id\": qa[\"query_id\"],\n",
    "        \"query\": qa[\"query\"],\n",
    "        \"ground_truth_answer\": qa[\"answers\"],\n",
    "        \"generated_answer\": response.content,\n",
    "        \"retrieved_chunks\": retrieved_chunks\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "print(\"The testing process is completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a169a6c-4870-4d53-9521-beea3ee1c04f",
   "metadata": {},
   "source": [
    "**Checkpoint -** Storing the results of the testing process of the RAG pipeline in a `.json` file, saving them, as a checkpoint, for the evaluation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f250627-1193-45c9-90ad-725f1a55a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Preprocessing of the results, since the retrieved chunks, which are `Document` objects, are not JSON serializable\n",
    "serializable_results = []\n",
    "\n",
    "for res in results:\n",
    "    serializable_res = res.copy()\n",
    "    serializable_res[\"retrieved_chunks\"] = [{\"page_content\": chunk.page_content} for chunk in res[\"retrieved_chunks\"]]\n",
    "    serializable_results.append(serializable_res)\n",
    "    \n",
    "# Saving the results list to a JSON file\n",
    "with open(\"./Output_Files/legalbench_rag_results.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(serializable_results, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0f29e-572d-44e2-b662-67ed16adc32c",
   "metadata": {},
   "source": [
    "## 5. RAG Evaluation\n",
    "\n",
    "### 5.1 RAGChecker\n",
    "\n",
    "Loading the RAG pipeline's test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d6996f9-cdc0-4991-b76e-a70943a62dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./Output_Files/legalbench_rag_results.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    json_results = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8524b5-f82a-424e-9fd6-a50ac8a00c75",
   "metadata": {},
   "source": [
    "RAGChecker requires that the RAG pipeline's results should be formatted as:\n",
    "\n",
    "```json\n",
    "{\n",
    "    results: [\n",
    "        {\n",
    "            \"query_id\": <query's identifier as a string value>,\n",
    "            \"query\": <The actual input query>,\n",
    "            \"gt_answer\": <The ground-truth answer provided in the dataset>,\n",
    "            \"response\": <RAG pipeline's generated response>,\n",
    "            \"retrieved_context\": [ <The list of the retrieved chunks, which are pertinent to the input  query>\n",
    "                {\n",
    "                    \"doc_id\": <The document identifier of the retrieved context passage as a string value>,\n",
    "                    \"text\": <The actual retrieved context passage>\n",
    "                },\n",
    "                {\n",
    "                    \"doc_id\": <The document identifier of the retrieved context passage as a string value>,\n",
    "                    \"text\": <The actual retrieved context passage>\n",
    "                }\n",
    "                ......\n",
    "            ]\n",
    "        }\n",
    "        ......\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "However, the RAG pipeline's results stored in `json_results.json` have the following format:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"query_id\": <The input query's identifier>,\n",
    "        \"query\": <The actual input query>,\n",
    "        \"ground_truth_answer\": <The ground-truth answer provided in the dataset>,\n",
    "        \"generated_answer\": <RAG pipeline's generated response>,\n",
    "        \"retrieved_chunks\": [\n",
    "            {\n",
    "                \"page_content\": <The context passage of the retrieved chunk>\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    .......\n",
    "]\n",
    "```\n",
    "\n",
    "Given that, a necessary preprocessing step is to convert the results' format to the exact format specified by RAGChecker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8aa5eb7-c70c-4f7a-80f0-491ad7b14470",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragchecker_results = {\"results\": []}\n",
    "\n",
    "for res in json_results:\n",
    "    if (len(res[\"ground_truth_answer\"]) > 0):\n",
    "        formatted_res = {\n",
    "            \"query_id\": str(res[\"query_id\"]),\n",
    "            \"query\": res[\"query\"],\n",
    "            \"gt_answer\": res[\"ground_truth_answer\"][0],\n",
    "            \"response\": res[\"generated_answer\"],\n",
    "            \"retrieved_context\": []\n",
    "        }\n",
    "    \n",
    "        for chunk in res[\"retrieved_chunks\"]:\n",
    "            formatted_res[\"retrieved_context\"].append({\"text\": chunk[\"page_content\"]})\n",
    "    \n",
    "        ragchecker_results[\"results\"].append(formatted_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c0837-8752-4225-bfe6-c0ba5b9bd49e",
   "metadata": {},
   "source": [
    "**Checkpoint -** Storing the RAG pipeline's results in the format required by RAGChecker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d02c55fc-d17d-4fc2-aacf-cc9f1300da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Output_Files/legalbench_rag_ragchecker.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(ragchecker_results, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce59e6-1a27-4d56-8d53-b0cb24f488ab",
   "metadata": {},
   "source": [
    "**RAGChecker Evaluation**\n",
    "\n",
    "RAGChecker computes a set of overall, retrieval, and generation metrics:\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| Precision | The fraction of correct generated claims $c_i^{(m)}$ in the generated response $m$ |\n",
    "| Recall | The fraction of ground-truth claims $c_i^{(gt)}$ that can be found in the model response |\n",
    "| F1 Score | The harmonic mean of Precision and Recall |\n",
    "| Claim Recall | The fraction of ground-truth claims $c^{(gt)}$ that can be found in the set of retrieved chunks $\\{chunk_j\\}$ |\n",
    "| Context Precision | The fraction of relevant chunks $\\{r\\text{-}chunk_j\\}$ from the $k$ retrieved chunks |\n",
    "| Faithfulness | The fraction of the model-generated claims $c_i^{(m)}$ that can be attributed to retrieved chunks. |\n",
    "| Relevant Noise Sensitivity | The fraction of generated claims $c_i^{(m)}$ that are incorrect and extracted from relevant retrieved chunks |\n",
    "| Irrelevant Noise Sensitivity | The fraction of generated claims $c_i^{(m)}$ that are incorrect and extracted from irrelevant retrieved chunks |\n",
    "| Hallucination | The fraction of generated claims $c_i^{(m)}$ that belong neither in the ground-truth answer $gt$ nor in any retrieved chunk |\n",
    "| Self Knowledge |  The fraction of generated responses $c_i^{(m)}$ that can be traced in the ground-truth $gt$ but not in any retrieved chunk |\n",
    "| Context Utilization | The fraction of ground-truth claims $c_i^{(gt)}$ that can be found in the set of retrieved chunks, that can also be extracted from the generated response $m$ |\n",
    "\n",
    "Executing the RAGChecker evaluation, and using `gpt-3.5-turbo` both as the extractor and the checker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "632cfe62-4916-4be9-a60e-716d97d882f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\61728\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "\u001b[32m2025-01-05 19:01:23.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragchecker.evaluator\u001b[0m:\u001b[36mextract_claims\u001b[0m:\u001b[36m113\u001b[0m - \u001b[1mExtracting claims for gt_answer of 100 RAG results.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: scikit-learn-intelex not installed, sklearn acceleration for the RepC checker is not enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m2025-01-05 19:02:01.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragchecker.evaluator\u001b[0m:\u001b[36mcheck_claims\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mChecking retrieved2answer for 100 RAG results.\u001b[0m\n",
      "\n",
      "\u001b[32m2025-01-05 19:04:55.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragchecker.evaluator\u001b[0m:\u001b[36mextract_claims\u001b[0m:\u001b[36m113\u001b[0m - \u001b[1mExtracting claims for response of 100 RAG results.\u001b[0m\n",
      "\n",
      "\u001b[32m2025-01-05 19:05:36.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragchecker.evaluator\u001b[0m:\u001b[36mcheck_claims\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mChecking answer2response for 100 RAG results.\u001b[0m\n",
      "\n",
      "\u001b[32m2025-01-05 19:05:56.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragchecker.evaluator\u001b[0m:\u001b[36mcheck_claims\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mChecking response2answer for 100 RAG results.\u001b[0m\n",
      "\n",
      "\u001b[32m2025-01-05 19:06:10.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragchecker.evaluator\u001b[0m:\u001b[36mcheck_claims\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mChecking retrieved2response for 100 RAG results.\u001b[0m\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 191/191 [03:18<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGResults(\n",
      "  100 RAG results,\n",
      "  Metrics:\n",
      "  {\n",
      "    \"overall_metrics\": {\n",
      "      \"precision\": 66.1,\n",
      "      \"recall\": 64.5,\n",
      "      \"f1\": 60.9\n",
      "    },\n",
      "    \"retriever_metrics\": {\n",
      "      \"claim_recall\": 82.6,\n",
      "      \"context_precision\": 91.6\n",
      "    },\n",
      "    \"generator_metrics\": {\n",
      "      \"context_utilization\": 75.6,\n",
      "      \"noise_sensitivity_in_relevant\": 26.3,\n",
      "      \"noise_sensitivity_in_irrelevant\": 0.7,\n",
      "      \"hallucination\": 6.8,\n",
      "      \"self_knowledge\": 0.8,\n",
      "      \"faithfulness\": 92.3\n",
      "    }\n",
      "  }\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ragchecker import RAGResults, RAGChecker\n",
    "from ragchecker.metrics import all_metrics\n",
    "\n",
    "# initialization of rag_results from json/dict\n",
    "with open(\"./Output_Files/legalbench_rag_ragchecker.json\") as fp:\n",
    "    rag_results = RAGResults.from_json(fp.read())\n",
    "\n",
    "# Setting up the evaluator using \"gpt-4o-mini\" as the extractor and checker model.\n",
    "evaluator = RAGChecker(\n",
    "    extractor_name=\"gpt-3.5-turbo\",\n",
    "    checker_name=\"gpt-3.5-turbo\",\n",
    "    batch_size_extractor=10,\n",
    "    batch_size_checker=10\n",
    ")\n",
    "\n",
    "# Evaluating results on all metrics, holistic, retrieval, and generation metrics\n",
    "evaluator.evaluate(rag_results, all_metrics)\n",
    "print(rag_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c23b67-1800-497e-9b0e-c3f5de4ee1f6",
   "metadata": {},
   "source": [
    "Storing the RAGChecker's evalaution results in a text file that will also store the evaluation results of the other 3 frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ac6ff37-4262-4059-8030-587e9c4841f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Output_Files/legalbench_rag_framework_results.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"RAGChecker results:\\n\\n\" + str(rag_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38050c-720c-467a-a0e6-6791a94b9d3a",
   "metadata": {},
   "source": [
    "### 5.2 RAGAs\n",
    "\n",
    "Due to dependency conflicts between packages required by RAGChecker and RAGAs, such as transformers and scikit-learn, RAGAs is executed on a different anaconda environment, that is installed and set up with the following commands:\n",
    "\n",
    "```\n",
    "conda create --name rag_eval_ragas python=3.11.5\n",
    "conda install jupyter\n",
    "pip install ragas \n",
    "```\n",
    "\n",
    "**The cells of this subsection should be executed on the \"rag_eval_ragas\" environment.**\n",
    "\n",
    "Activating the environment with the command:\n",
    "```\n",
    "conda activate rag_eval_ragas\n",
    "```\n",
    "\n",
    "Loading the RAG pipeline's test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e94e649-928c-45e2-9b1d-82748827d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./Output_Files/legalbench_rag_results.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    json_results = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67406b2",
   "metadata": {},
   "source": [
    "**RAGAs Evaluation**\n",
    "\n",
    "RAGAs provides a series of RAG evaluation metrics as well as general LLM evaluation metrics, but in this setting, I chose to compute the subset of those metrics that are described in the thesis report:\n",
    "\n",
    "| RAG Metric | Description |\n",
    "|--------|-------------|\n",
    "| Faithfulness | The fraction of inferred claims $|V|$ from the total claims $|S|$ |\n",
    "| Answer Relevance | The average cosine similarity of the input query and a set of LLM-generated queries that can possibly result in the same generated answer |\n",
    "| Context Relevance | The fraction of the sentences in the context the successfully confront the question |\n",
    "| Context Precision@$K$ | The fraction of contextually relevant retrieved chunks - Average of Precision@$k$ for all $k$ up to $K$ |\n",
    "| Context Recall | The fraction of relevant chunks that were actually retrieved based on the input query |\n",
    "| Context Entity Recall | Examines how many of the ground-truth entities were present in the generated answer |\n",
    "| Relevant/Irrelevant Noise Sensitivity | Same metrics as RAGChecker |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8629354d-2212-4c30-a025-a9fcc3c33b1b",
   "metadata": {},
   "source": [
    "| General LLM Metric | Description |\n",
    "|--------------------|-------------|\n",
    "| Non-LLM Semantic Similarity | The similarity (on a scale of 0 to 1) between the model response and the ground-truth answer using distance measurements|\n",
    "| BLEU | The similarity (on a scale of 0 to 1) between the model response and the ground-truth answer using the n-gram precision |\n",
    "| ROUGE | The fraction of overlap between the generated and reference response based on the n-gram precision, recall and F1 score |\n",
    "| Exact Match | Tests whether the generated response is exactly the same (1), or not (0), as the ground-truth answer |\n",
    "| String Presence | Checks whether the generated response contains (1), or not (0), several parts or keywords of the reference answer |\n",
    "\n",
    "However, just like the MS MARCO dataset, calculating all those metrics for 100 RAG result instances is a resource-intensive process, requiring almost 10 minutes per instance!\n",
    "\n",
    "That's why I decided only to calcuate the 3 most fundamental metrics: **Faithfulness**, **Answer Relevance** (defined as response relevancy in the code), and **Context Relevance** (defined as context recall in the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0c40b3f-5f4b-48fc-a58d-e68d6943ee5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the RAG pipeline results using RAGAS....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [44:55<00:00, 26.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The evaluation process has been completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing RAG metrics\n",
    "from ragas.metrics import Faithfulness, ResponseRelevancy, LLMContextRecall\n",
    "\n",
    "# Defining the LLM and Embeddings model that are required parameters for some evaluation metrics\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# The LLM and Embeddings model need to be wrapped so that they conform with RAGAs' interface\n",
    "langchain_llm = LangchainLLMWrapper(langchain_llm=llm)\n",
    "langchain_embeddings = LangchainEmbeddingsWrapper(embeddings=embeddings)\n",
    "\n",
    "# Defining all the scoring functions that leverage the LLM to compute the respective evaluation metric \n",
    "faithfulness = Faithfulness(llm=langchain_llm)\n",
    "answer_relevance = ResponseRelevancy(llm=langchain_llm, embeddings=langchain_embeddings)\n",
    "context_relevance = LLMContextRecall(llm=langchain_llm)\n",
    "\n",
    "\n",
    "# Initializing Metrics dictionary\n",
    "# In every iteration of the evaluation process, \n",
    "# this dictionary will hold the running sum of each metric for the evaluated dataset instances up to this point,\n",
    "# and, in the end, will store the average value for every metric\n",
    "metrics =  {\n",
    "    \"faithfulness\" : 0,\n",
    "    \"answer_relevance\" : 0,\n",
    "    \"context_relevance\" : 0\n",
    "}\n",
    "\n",
    "# Main evaluation loop\n",
    "print(\"Evaluating the RAG pipeline results using RAGAS....\")\n",
    "\n",
    "for rag_result in tqdm(json_results):\n",
    "\n",
    "    # Each rag result instance must be converted into a single-turn sample instance\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=rag_result[\"query\"],\n",
    "        response=rag_result[\"generated_answer\"],\n",
    "        reference=rag_result[\"ground_truth_answer\"][0], # ground_truth_answer is a list and its first element is the actual string value\n",
    "        retrieved_contexts=[context[\"page_content\"] for context in rag_result[\"retrieved_chunks\"]]\n",
    "    )\n",
    "\n",
    "    # calculating the metrics for each rag result instance and storing the running sum to the 'metrics' dicitionary\n",
    "    metrics[\"faithfulness\"] += await faithfulness.single_turn_ascore(sample)\n",
    "    metrics[\"answer_relevance\"] += await answer_relevance.single_turn_ascore(sample)\n",
    "    metrics[\"context_relevance\"] += await context_relevance.single_turn_ascore(sample)\n",
    "\n",
    "# After the evaluation loop is completed, i compute the average of each metric on the 100 rag result instances.\n",
    "for metric_label in list(metrics.keys()):\n",
    "    metrics[metric_label] /= 100\n",
    "\n",
    "print(\"The evaluation process has been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0e0596c-1113-4aff-94ed-2e5d7337ed22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RAGAs average metrics for the sample of 100 RAG results are:\n",
      " {'faithfulness': 0.8721074718643704, 'answer_relevance': 0.7764295769799838, 'context_relevance': 0.5683333333333332}\n"
     ]
    }
   ],
   "source": [
    "print(f\"The RAGAs average metrics for the sample of 100 RAG results are:\\n {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23345235-c578-4475-a26f-844ea27065e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Output_Files/legalbench_rag_framework_results.txt\", \"a+\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\\nRAGAs results:\\n\\n\" + str(metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
